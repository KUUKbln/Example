<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <title>Akustische Bitmap-√úbertragung (Demo)</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 2em;
      background: #f8f8f8;
    }
    canvas {
      border: 1px solid #ccc;
      image-rendering: pixelated;
      width: 512px;
      height: 512px;
    }
    details {
      margin-bottom: 1.5em;
      background: #fff;
      padding: 1em;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    summary {
      font-weight: bold;
      font-size: 1.1em;
      cursor: pointer;
    }
    .toolbar {
      margin: 0.5em 0;
    }
    .toolbar button {
      margin-right: 10px;
    }
    #debugLog {
      font-family: monospace;
      white-space: pre-wrap;
      background: #eee;
      padding: 0.5em;
      max-height: 200px;
      overflow-y: auto;
    }
  </style>
</head>
<body>

<!-- SETTINGS -->
<details open>
  <summary>‚öôÔ∏è Einstellungen</summary>
  <textarea id="settingsEditor" rows="12" style="width: 100%; font-family: monospace;"></textarea>
  <br>
  <button onclick="applySettings()">üîÅ Update Einstellungen</button>
  <div id="settingsStatus" style="margin-top: 0.5em; font-weight: bold;"></div>
</details>

<!-- SENDER & EMPF√ÑNGER UI -->
<details open>
  <summary>üì§ Sender & Empf√§nger</summary>
  <canvas id="canvasSender" width="512" height="512"></canvas>
  <canvas id="canvasReceiver" width="512" height="512"></canvas>
  <div id="senderStatus">Sender: Bereit</div>
  <div id="receiverStatus">Empf√§nger: Inaktiv</div>
  
  <div class="toolbar">
    <label for="transmissionMode"><strong>Modus:</strong></label>
    <select id="transmissionMode" onchange="ontransmissionModeChange()">
      <option value="digital">üî¢ Digital (Byteweise)</option>
      <option value="analog">üéöÔ∏è Analog (Zeilenweise)</option>
    </select>
    <label for="upload"><strong>Bild:</strong></label>
    <input type="file" id="upload" accept="image/*">
  </div>
  
  <div class="toolbar">
    <button onclick="startTransmission()">‚ñ∂Ô∏è Senden</button>
    <button onclick="stopTransmission()">‚èπÔ∏è Stopp</button>
    <button onclick="resetTransmission()">üîÑ Zur√ºcksetzen</button>
    <button onclick="exportWavFromCurrentImage()">üíæ Exportiere als WAV</button>
  </div>
  
  <div class="toolbar">
    <button onclick="sendByte(0xFF)">Sende 0xFF</button>
    <button onclick="sendByte(0x00)">Sende 0x00</button>
    <button onclick="sendByte(0xAB)">Sende 0xAB</button>
    <input type="text" id="customByteInput" placeholder="Hex z.‚ÄØB. AB" size="6">
    <button onclick="sendCustomByte()">Sende eigenes Byte</button>
  </div>
  <br/>
  <div class="toolbar">
    <button onclick="startReceiver(false)">üéß Empfang (intern)</button>
    <button onclick="startReceiver(true)">üéôÔ∏è Empfang (Mikrofon)</button>
    <button onclick="stopReceiver()">‚èπÔ∏è Stopp</button>
    <button onclick="resetReceiver()">üîÑ Zur√ºcksetzen</button>
  </div>
</details>

<!-- DEBUG / SPEKTRUM -->
<details>
  <summary>üõ† Debug / Spektrum</summary>

  <canvas id="receiverWaterfall" width="512" height="100" style="border:1px solid #ccc;"></canvas>
  <div id="debugLog"></div>
  <canvas id="receiverSpectrum" width="512" height="100" style="border:1px solid #ccc;"></canvas>
</details>

<script>
/** === Einstellungen & Globals === **/
const settings = {
  canvasSize: 512,
  numFrequencies: 16,
  numTones: 4,
  baseFreq: 220,
  freqStep: 150,
  symbolDuration: 100, // in ms
  gain: 0.3,
  showCursor: true,
  transmissionMode: 'analog',  // Standard-Modus

  debug: false
};

const settingsDummy = {
  "üé∂numTones": "4",
  "‚è≥symbolDuration": "200",
  "üñºÔ∏ècanvasSize": 512,
  "üì∂numFrequencies": 16,
  "üéµbaseFreq": 220,
  "üî¢freqStep": 150,
  "üîägain": 0.3,
  "üñ±Ô∏èshowCursor": true,
  "üì°transmissionMode": "digital",
  "üêûdebug": false
};

// √úbertragungs-Modi
const MODES = {
  DIGITAL: 'digital',
  ANALOG: 'analog',
};

let transmissionMode = MODES.DIGITAL;

function ontransmissionModeChange() {
  const select = document.getElementById("transmissionMode");
  const mode = select.value;
  if (mode === MODES.DIGITAL || mode === MODES.ANALOG) {
    settings.transmissionMode = mode;
    log(`üîÄ Modus ge√§ndert: ${mode}`);
    document.getElementById("senderStatus").textContent = `Sender: Modus = ${mode}`;
  }
  
  drawGreyscale(ctxSender, grayscaleBitmap, settings.canvasSize, -1);
}


const settingsEditor = document.getElementById("settingsEditor");
const settingsStatus = document.getElementById("settingsStatus");

function refreshSettingsEditor() {
  settingsEditor.value = JSON.stringify(settings, null, 2);
}
refreshSettingsEditor();

/** Audio / Analyse Setup **/
let audioCtx = new (window.AudioContext || window.webkitAudioContext)();
let gainNode = audioCtx.createGain();
gainNode.gain.value = settings.gain;
gainNode.connect(audioCtx.destination);

let sharedAnalyser = audioCtx.createAnalyser();
sharedAnalyser.fftSize = 2048;
let sharedFreqData = new Float32Array(sharedAnalyser.frequencyBinCount);

// F√ºr internen Loopback (Sender ‚Üí Empf√§nger intern)
let mediaStreamDest = audioCtx.createMediaStreamDestination();
// Quelle, die den internen Stream erzeugt
let mediaStreamSource = audioCtx.createMediaStreamSource(mediaStreamDest.stream);

/** Canvas-Kontexte **/
let ctxSender, ctxReceiver;

window.onload = () => {
  ctxSender = document.getElementById("canvasSender").getContext("2d");
  ctxReceiver = document.getElementById("canvasReceiver").getContext("2d");
  clearCanvas(ctxSender);
  clearCanvas(ctxReceiver);
  log("üü¢ System bereit");
  //document.getElementById("transmissionMode").value = transmissionMode;
  document.getElementById("transmissionMode").value = settings.transmissionMode;
  document.getElementById("senderStatus").textContent = `Sender: Modus = ${settings.transmissionMode}`;

};

/** Logging **/
function log(msg) {
  if (settings.debug) {
    const logElem = document.getElementById("debugLog");
    logElem.textContent += msg + "\n";
    logElem.scrollTop = logElem.scrollHeight;
  }
  console.log("LOG:", msg);
}





/** Settings anwenden **/
async function applySettings() {
  try {
    const updated = JSON.parse(settingsEditor.value);
    Object.assign(settings, updated);

    stopTransmission();
    stopReceiver();

    if (audioCtx && audioCtx.state !== "closed") {
      await audioCtx.close();
    }
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();

    // Rebuild audio graph
    gainNode = audioCtx.createGain();
    gainNode.gain.value = settings.gain;
    gainNode.connect(audioCtx.destination);

    sharedAnalyser = audioCtx.createAnalyser();
    sharedAnalyser.fftSize = 2048;
    sharedFreqData = new Float32Array(sharedAnalyser.frequencyBinCount);

    // Neuer mediaStreamDest / Source
    mediaStreamDest = audioCtx.createMediaStreamDestination();
    mediaStreamSource = audioCtx.createMediaStreamSource(mediaStreamDest.stream);

    resetTransmission();
    resetReceiver();

    //refreshSettingsEditor();
    updateUI();
    settingsStatus.textContent = "‚úÖ Einstellungen √ºbernommen & System neu initialisiert.";
    settingsStatus.style.color = "green";
    log("‚öôÔ∏è Einstellungen aktualisiert");

  } catch (err) {
    settingsStatus.textContent = "‚ùå Ung√ºltiges JSON!";
    settingsStatus.style.color = "red";
    log("Fehler beim Einlesen der Einstellungen: " + err);
  }
}

/** Bitmap / Sender Logik **/
let bitmapBytes = new Uint8Array((settings.canvasSize ** 2) / 8);
let grayscaleBitmap = []; // Array of Zeilen, jede Zeile = Array von Graustufen (0‚Äì255)

let currentByteIndex = 0;
let transmissionRunning = false;

document.getElementById("upload").addEventListener("change", handleImageUpload);

function handleImageUpload(event) {
  const file = event.target.files[0];
  if (!file) return;

  const img = new Image();
  img.onload = () => {
    const temp = document.createElement("canvas");
    temp.width = settings.canvasSize;
    temp.height = settings.canvasSize;
    const tctx = temp.getContext("2d");
    tctx.drawImage(img, 0, 0, settings.canvasSize, settings.canvasSize);
    const imgData = tctx.getImageData(0, 0, settings.canvasSize, settings.canvasSize).data;

    bitmapBytes = new Uint8Array((settings.canvasSize ** 2) / 8);
    grayscaleBitmap = [];

    for (let y = 0; y < settings.canvasSize; y++) {
      const row = [];
      for (let x = 0; x < settings.canvasSize; x++) {
        const idx = (y * settings.canvasSize + x);
        const i = idx * 4;
        const r = imgData[i], g = imgData[i+1], b = imgData[i+2];
        const gray = (r + g + b) / 3;
        row.push(gray);
        // Digital: Bit setzen
        const bit = gray < 128 ? 1 : 0;
        const byteIndex = Math.floor(idx / 8);
        const bitIndex = 7 - (idx % 8);
        bitmapBytes[byteIndex] |= (bit << bitIndex);
      }
      grayscaleBitmap.push(row);
    }
    drawGreyscale(ctxSender, grayscaleBitmap, settings.canvasSize);
    //drawBitmap(ctxSender, bitmapBytes, settings.canvasSize);
    
    document.getElementById("senderStatus").textContent = "Sender: Bild geladen";
    log("Bild geladen und verarbeitet");
  };
  img.src = URL.createObjectURL(file);
}

function drawGreyscale(ctx, greyscaleArray, size , cursorPosition = -1) {
  const imageData = ctx.createImageData(size, size);

  for (let y = 0; y < size; y++) {
    for (let x = 0; x < size; x++) {
      const gray = greyscaleArray[y][x];
      const i = (y * size + x) * 4;
      imageData.data[i] = gray;      // R
      imageData.data[i + 1] = gray;  // G
      imageData.data[i + 2] = gray;  // B
      imageData.data[i + 3] = 255;   // A
    }
  }
  ctx.putImageData(imageData, 0, 0);
  
  // ‚úÖ Cursor einf√ºgen, falls aktiviert
  if (settings.showCursor && typeof cursorPosition === 'number' && cursorPosition >= 0) {
    console.log("draw analog cursor@"+cursorPosition);
    ctx.save();
    ctx.strokeStyle = 'red';
    ctx.lineWidth = 1;
    ctx.beginPath();
    ctx.moveTo(0, cursorPosition + 0.5);
    ctx.lineTo( size, cursorPosition+ 0.5);
    ctx.stroke();
    ctx.restore();
  }

}


function drawBitmap(ctx, bytes, size, cursorByte = -1) {
  const imageData = ctx.createImageData(size, size);
  for (let i = 0; i < size * size; i++) {
    const byteIndex = Math.floor(i / 8);
    const bitIndex = 7 - (i % 8);
    const bit = (bytes[byteIndex] >> bitIndex) & 1;
    const val = bit ? 0 : 255;
    imageData.data[i*4 + 0] = val;
    imageData.data[i*4 + 1] = val;
    imageData.data[i*4 + 2] = val;
    imageData.data[i*4 + 3] = 255;
  }
  ctx.putImageData(imageData, 0, 0);

  if (settings.showCursor && cursorByte >= 0) {
    const pxIdx = cursorByte * 8;
    const x = pxIdx % size;
    const y = Math.floor(pxIdx / size);
    ctx.strokeStyle = "red";
    ctx.lineWidth = 1;
    if (x + 8 <= size) {
      ctx.strokeRect(x - 0.5, y - 0.5, 8, 1);
    } else {
      for (let i = 0; i < 8; i++) {
        const px = (x + i) % size;
        const py = y + Math.floor((x + i) / size);
        ctx.strokeRect(px - 0.5, py - 0.5, 1, 1);
      }
    }
  }
}

const waterfallCanvas = document.getElementById("receiverWaterfall");
const waterfallCtx = waterfallCanvas.getContext("2d");

// H√∂he in Pixel f√ºr eine Zeile im Wasserfall
const lineHeight = 2;

function drawWaterfallLine(freqData) {
  //console.log("drawWaterfallLine res:"+freqData);
  // 1. Canvas nach unten verschieben um eine Zeile
  waterfallCtx.drawImage(
    waterfallCanvas,
    0, 0, waterfallCanvas.width, waterfallCanvas.height - lineHeight,
    0, lineHeight, waterfallCanvas.width, waterfallCanvas.height - lineHeight
  );

  // 2. Neue Zeile oben zeichnen
  const width = waterfallCanvas.width;
  const barW = width / freqData.length;

  for (let i = 0; i < freqData.length; i++) {
    const v = freqData[i];
    // Farbe basierend auf Intensit√§t (hier grau-Werte)
    waterfallCtx.fillStyle = `rgb(${v},${v},${v})`;
    waterfallCtx.fillRect(i * barW, 0, barW, lineHeight);
  }
}


function startTransmission() {
  if (transmissionRunning) return;
  transmissionRunning = true;

  if (settings.transmissionMode === MODES.DIGITAL) {
    document.getElementById("senderStatus").textContent = "Sender: Digital-Sendung l√§uft...";
    (async () => {
      for (let i = 0; i < bitmapBytes.length; i++) {
        if (!transmissionRunning) break;
        const byte = bitmapBytes[i];
        const freqs = encodeByteToFrequencies(byte);
        playLiveFrequencies(freqs, settings.symbolDuration);
        drawBitmap(ctxSender, bitmapBytes, settings.canvasSize, i);
        const pct = ((i+1)/bitmapBytes.length * 100).toFixed(1);
        document.getElementById("senderStatus").textContent =
          `Sender: Byte ${i+1}/${bitmapBytes.length} (${pct}%)`;
        await new Promise(r => setTimeout(r, settings.symbolDuration));
      }
      transmissionRunning = false;
      document.getElementById("senderStatus").textContent = "Sender: Fertig";
      log("Digitale √úbertragung abgeschlossen");
    })();
  }

  else if (settings.transmissionMode === MODES.ANALOG) {
    document.getElementById("senderStatus").textContent = "Sender: Analoge Zeilen√ºbertragung...";
    const lineDuration = settings.symbolDuration ; // z.B. l√§nger halten f√ºr Zeile
    (async () => {
      for (let y = 0; y < grayscaleBitmap.length; y++) {
        if (!transmissionRunning) break;
        const row = grayscaleBitmap[y];
        const { frequencies, gains } = encodeLineToFrequencies(row);
        playLiveFrequencies(frequencies, lineDuration, gains);
        document.getElementById("senderStatus").textContent =
          `Sender: Zeile ${y+1}/${grayscaleBitmap.length}`;
        await new Promise(r => setTimeout(r, lineDuration + 20));
        drawGreyscale(ctxSender, grayscaleBitmap, settings.canvasSize, y);
      }
      transmissionRunning = false;
      document.getElementById("senderStatus").textContent = "Sender: Fertig";
      log("Analoge √úbertragung abgeschlossen");
    })();
  }
  
  // NachTransmisson, greyscale Zeichen
  drawGreyscale(ctxSender, grayscaleBitmap, settings.canvasSize);
}

function stopTransmission() {
  transmissionRunning = false;
  document.getElementById("senderStatus").textContent = "Sender: Gestoppt";
  log("Senden gestoppt");
}

function resetTransmission() {
  currentByteIndex = 0;
  drawBitmap(ctxSender, bitmapBytes, settings.canvasSize);
  document.getElementById("senderStatus").textContent = "Sender: Zur√ºckgesetzt";
}

/** Erzeuge T√∂ne, verbinde mit Analyser & StreamDestination **/
//function playLiveFrequencies(frequencies, durationMs) {
function playLiveFrequencies(frequencies, durationMs, gains = [], defaultGain = settings.gain) {
  const now = audioCtx.currentTime;
  const durationSec = durationMs / 1000;
  
  if(false){
    console.log("playLiveFrequencies:")
    console.log(frequencies)
    console.log(gains)
  }


  frequencies.forEach((freq, i) => {
    const osc = audioCtx.createOscillator();
    osc.type = "sine";
    osc.frequency.value = freq;

    const individualGain = typeof gains[i] === "number" ? gains[i] : defaultGain;

    const fade = audioCtx.createGain();
    fade.gain.setValueAtTime(0, now);
    fade.gain.linearRampToValueAtTime(individualGain, now + 0.01);
    fade.gain.setValueAtTime(individualGain, now + durationSec - 0.02);
    fade.gain.linearRampToValueAtTime(0, now + durationSec);

    // Audio Routing
    osc.connect(fade);
    fade.connect(gainNode);          // global gain (optional)
    fade.connect(sharedAnalyser);    // Spektrumanzeige
    fade.connect(mediaStreamDest);   // Loopback

    osc.start(now);
    osc.stop(now + durationSec);
  });
}



function sendCustomByte() {
  const hex = document.getElementById("customByteInput").value.trim();
  if (!/^[0-9a-fA-F]{1,2}$/.test(hex)) {
    log("‚ùå Ung√ºltiges Hex-Byte");
    return;
  }
  const byte = parseInt(hex, 16);
  sendByte(byte);
}


function sendByte(byte) {
  const freqs = encodeByteToFrequencies(byte);
  playLiveFrequencies(freqs, settings.symbolDuration);
  log("üîä Einzel-Byte gesendet: 0x" + byte.toString(16).padStart(2, '0'));
}



/** Empfang / Dekodierung **/
let recvRunning = false;
let receiveBitmapBytes;
let receiveBytePos = 0;
let startDetected = false;

async function startReceiver(useMic = false) {
  if (recvRunning) return;

  if (useMic) {
    try {
      const audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 44100,
          channelCount: 1,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });
      const src = audioCtx.createMediaStreamSource(audioStream);
      src.connect(sharedAnalyser);
      log("üì° Mikrofonquelle verbunden");
    } catch (err) {
      log("‚ùå Mikrofonzugriff fehlgeschlagen: " + err.message);
      document.getElementById("receiverStatus").textContent = "Empf√§nger: Mikrofonfehler";
      return;
    }
  }

  recvRunning = true;
  startDetected = false;
  receiveBytePos = 0;
  drawSpectrum();
  receiveBitmapBytes = new Uint8Array((settings.canvasSize ** 2) / 8);
  clearCanvas(ctxReceiver);
  document.getElementById("receiverStatus").textContent = "Empf√§nger: Warte";

  receptionLoop();
}

function drawSpectrum() {
  if (!recvRunning) return;

  const freq = new Uint8Array(sharedAnalyser.frequencyBinCount);
  sharedAnalyser.getByteFrequencyData(freq);

  const canvas = document.getElementById("receiverSpectrum");
  const ctx = canvas.getContext("2d");
  ctx.clearRect(0, 0, canvas.width, canvas.height);

  const barW = canvas.width / freq.length;
  for (let i = 0; i < freq.length; i++) {
    const v = freq[i];
    ctx.fillStyle = `rgb(${v},${v},${v})`;
    ctx.fillRect(i * barW, canvas.height - v, barW, v);
  }

  if (recvRunning) {
    requestAnimationFrame(drawSpectrum);
  }
}

const maxWaterfallLines = 500;  // Beispiel, gr√∂√üer als vorher
let waterfallBuffer = []; // Array f√ºr Frequenz-Sets

async function receptionLoop() {
  const symbolMs = settings.symbolDuration;
  const lowestFreq = 55;
  const highestFreq = 8000;
  
  while (recvRunning) {
    await new Promise(r => setTimeout(r, symbolMs));
    sharedAnalyser.getFloatFrequencyData(sharedFreqData);

    const detected = detectFrequencies(sharedFreqData, audioCtx.sampleRate, sharedAnalyser.fftSize);
    //log("Empf: detektiert Frequenzen (raw): " + detected.map(f => f.toFixed(1)).join(","));
    const binWidth_hz = audioCtx.sampleRate / sharedAnalyser.fftSize;
    // uns interessiert nur der Frquenzbereich  lowestFreq bis highestFreq
    const bin_low = Math.floor(lowestFreq/ binWidth_hz);
    const bin_high = Math.floor(highestFreq/ binWidth_hz);
    const detectedInRange = detected.filter(freq => freq >= lowestFreq && freq <= highestFreq);

    
    if (!startDetected) {
      if (isStartSymbol(detected)) {
        startDetected = true;
        document.getElementById("receiverStatus").textContent = "Empf√§nger: Start erkannt";
        log("üü¢ Start erkannt");
      }
      //continue;
    }

    if (isStopSymbol(detected)) {
      document.getElementById("receiverStatus").textContent = "Empf√§nger: Stop erkannt";
      log("üî¥ Stop erkannt");
      //recvRunning = false;
      //continue;
      //break;
    }

    if (detected.length !== settings.numTones) {
      log("‚ö†Ô∏è Falsche Anzahl T√∂ne: " + detected.length);
      //continue;
    }

    // Wasserfall speichern, √§lteste Zeile entfernen, wenn zu gro√ü
    if (waterfallBuffer.length >= maxWaterfallLines) {
      waterfallBuffer.shift();
    }
    waterfallBuffer.push(detectedInRange);
  console.log(detectedInRange);  
    // Draw Waterfall
    const freqByteData = new Uint8Array(sharedAnalyser.frequencyBinCount);
    sharedAnalyser.getByteFrequencyData(freqByteData);
    const freqByteDataInRange = freqByteData.slice(bin_low, bin_high + 1);
    drawWaterfallLine(freqByteDataInRange);
console.log("->drawWaterfallLine:"+freqByteData.length);



    // Beispiel: Dekodiere immer nur die letzte Zeile in Byte
    const byte = decodeFrequenciesToByte(detected);
    if (byte == null) {
      log("‚ö†Ô∏è Byte nicht dekodierbar");
      continue;
    }

    receiveBitmapBytes[receiveBytePos] = byte;
    receiveBytePos++;

    drawBitmap(ctxReceiver, receiveBitmapBytes, settings.canvasSize);
    document.getElementById("receiverStatus").textContent =
      `Empf√§nger: Byte ${receiveBytePos}/${receiveBitmapBytes.length}`;

    if (receiveBytePos >= receiveBitmapBytes.length) {
      recvRunning = false;
      document.getElementById("receiverStatus").textContent = "Empf√§nger: Fertig";
      log("‚úÖ Empfang komplett");
      break;
    }
  }
}

function stopReceiver() {
  recvRunning = false;
  document.getElementById("receiverStatus").textContent = "Empf√§nger: Stopp";
  log("Empf√§nger gestoppt");
}

function resetReceiver() {
  recvRunning = false;
  clearCanvas(ctxReceiver);
  receiveBitmapBytes = null;
  receiveBytePos = 0;
  document.getElementById("receiverStatus").textContent = "Empf√§nger: Zur√ºckgesetzt";
  log("Empf√§nger zur√ºckgesetzt");
}

/** Frequenzkodierung & Dekodierung **/
function encodeByteToFrequencies(byte) {
  const freqs = [];
  for (let i = 0; i < settings.numTones; i++) {
    const part = (byte >> (i * 2)) & 0b11;
    const index = i * 4 + part;
    freqs.push(settings.baseFreq + index * settings.freqStep);
  }
  return freqs;
}
function encodeLineToFrequencies(lineArray) {
  const baseFreq = 300;
  const freqStep = 100;
  const numAnalogTones =64;

  const segmentSize = Math.floor(lineArray.length / numAnalogTones);

  const frequencies = [];
  const gains = [];

  for (let i = 0; i < numAnalogTones; i++) {
    const start = i * segmentSize;
    const end = (i === numAnalogTones - 1) ? lineArray.length : start + segmentSize;
    
    // Mittelwert der Graustufen im Segment berechnen
    let sum = 0;
    for (let j = start; j < end; j++) {
      sum += lineArray[j];
    }
    const avgGray = sum / (end - start);
    
    frequencies.push(baseFreq + i * freqStep);
    gains.push(avgGray / 255);
  }

  return { frequencies, gains };
}


function detectFrequencies(freqData, sampleRate, fftSize) {
  const candidates = [];
  const thresholdPerFreq = {
    500: -82, 650: -82, 800: -83, 950: -83,
    1100: -84, 1250: -85, 1400: -86, 1550: -87,
    1700: -88, 1850: -88, 2000: -89, 2150: -89,
    2300: -90, 2450: -90, 2600: -91
  };

  for (let i = 0; i < settings.numFrequencies; i++) {
    const freq = settings.baseFreq + i * settings.freqStep;
    const bin = Math.round(freq / sampleRate * fftSize);
    const mag = freqData[bin];
    const thr = thresholdPerFreq[freq] ?? -90;
    if (mag > thr) {
      candidates.push({ freq, magnitude: mag });
    }
  }

  candidates.sort((a,b) => b.magnitude - a.magnitude);
  const strongest = candidates.slice(0, settings.numTones);
  return strongest.map(c => c.freq);
}

function isStartSymbol(freqs) {
  const start = encodeByteToFrequencies(0xFF);
  return arraysEqualUnordered(freqs, start);
}
function isStopSymbol(freqs) {
  return false;
  //const stop = encodeByteToFrequencies(0x100);
  //return arraysEqualUnordered(freqs, stop);
}

function decodeFrequenciesToByte(freqs) {
  if (!Array.isArray(freqs) || freqs.length !== settings.numTones) return null;
  let byte = 0;
  for (let i = 0; i < freqs.length; i++) {
    const freq = freqs[i];
    const rel = freq - settings.baseFreq;
    const idx = Math.round(rel / settings.freqStep);
    if (idx < 0 || idx >= settings.numFrequencies) return null;
    const block = Math.floor(idx / 4);
    const value = idx % 4;
    if (block < 0 || block >= settings.numTones) return null;
    byte |= (value << (block * 2));
  }
  return byte;
}

function arraysEqualUnordered(a, b) {
  if (a.length !== b.length) return false;
  const sa = a.slice().sort();
  const sb = b.slice().sort();
  return sa.every((v,i) => v === sb[i]);
}
function clearCanvas(ctx) {
  ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
}


</script>

<script>


  /** === WAV-Export === **/

  let isBusyGeneratingWAV = false;
  
  function generateWavBufferFromBytes(bytes) {
    isBusyGeneratingWAV = true;
    const sampleRate = 44100;
    const durationSec = bytes.length * (settings.symbolDuration / 1000);
    const totalSamples = Math.floor(sampleRate * durationSec);

    const samples = new Float32Array(totalSamples);
    let writeIndex = 0;
    const samplesPerSymbol = Math.floor(sampleRate * (settings.symbolDuration / 1000));

    for (let i = 0; i < bytes.length; i++) {
      const byte = bytes[i];
      const freqs = encodeByteToFrequencies(byte);
      for (let j = 0; j < samplesPerSymbol; j++) {
        let t = j / sampleRate;
        let sample = 0;
        freqs.forEach(freq => {
          sample += Math.sin(2 * Math.PI * freq * t);
        });
        sample /= freqs.length;
        sample *= settings.gain;
        samples[writeIndex++] = sample;
      }
    }
    return samples;
  }

  function floatTo16BitPCM(floatSamples) {
    const buffer = new Int16Array(floatSamples.length);
    for (let i = 0; i < floatSamples.length; i++) {
      const s = Math.max(-1, Math.min(1, floatSamples[i]));
      buffer[i] = s * 0x7FFF;
    }
    return buffer;
  }

  function createWavBlob(samples, sampleRate = 44100) {
    const pcm16 = floatTo16BitPCM(samples);
    const buffer = new ArrayBuffer(44 + pcm16.length * 2);
    const view = new DataView(buffer);

    const writeStr = (offset, str) => {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    };

    writeStr(0, 'RIFF');
    view.setUint32(4, 36 + pcm16.length * 2, true);
    writeStr(8, 'WAVE');
    writeStr(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true); // PCM
    view.setUint16(22, 1, true); // Mono
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeStr(36, 'data');
    view.setUint32(40, pcm16.length * 2, true);

    for (let i = 0; i < pcm16.length; i++) {
      view.setInt16(44 + i * 2, pcm16[i], true);
    }

    return new Blob([view], { type: 'audio/wav' });
  }

  function exportWavFromCurrentImage() {

    if (isBusyGeneratingWAV) {
      alert("‚ö†Ô∏è WAV wird bereits erstellt");
      return;
    }
    if (!bitmapBytes || bitmapBytes.length === 0) {
      alert("‚ö†Ô∏è Kein Bild geladen");
      return;
    }

    const floatSamples = generateWavBufferFromBytes(bitmapBytes);
    const wavBlob = createWavBlob(floatSamples);

    const url = URL.createObjectURL(wavBlob);
    const a = document.createElement("a");
    a.href = url;
    a.download = "bitmap-akustisch.wav";
    a.click();
    URL.revokeObjectURL(url);
    isBusyGeneratingWAV = false;
    log("üéµ WAV-Datei erzeugt und zum Download angeboten");
  }

</script>

<script>

    // Settings, Params, QueryString
    
    // Query aus URL auslesen und settings dynamisch √ºberschreiben
    function applyQueryParamsToSettings() {
      const params = new URLSearchParams(window.location.search);
      for (const key of Object.keys(settings)) {
        if (params.has(key)) {
          settings[key] = params.get(key);
          console.log("applyQueryParamsToSettings: "+key+","+settings[key]);
        }
      }
    }
    
     // UI mit settings synchronisieren (dynamisch)
    function updateUI() {
      for (const key of Object.keys(settings)) {
        const element = document.getElementById(key);
        if (element) {
          element.value = settings[key];
          console.log("updateUI,"+key+":"+element.value)
        }
      }
      refreshSettingsEditor();
    }

     // URL Query string dynamisch aktualisieren (ohne Reload)
   function updateURLQueryFromSettings() {
    const currentParams = new URLSearchParams(window.location.search);
    
    // Nur vorhandene Query-Parameter aktualisieren
    for (const key of Object.keys(settings)) {
      if (currentParams.has(key)) {
        currentParams.set(key, settings[key]);
      }
    }
    
    const newRelativePathQuery = window.location.pathname + (currentParams.toString() ? '?' + currentParams.toString() : '');
    history.replaceState(null, '', newRelativePathQuery);
  }

    
     // Event-Handler dynamisch setzen (f√ºr Inputs und Selects)
    function setupEventListeners() {
      for (const key of Object.keys(settings)) {
        const element = document.getElementById(key);
        if (element) {
          element.addEventListener('input', e => {
            settings[key] = e.target.value;
            updateURLQueryFromSettings();
          });
          // F√ºr Selects passt auch 'change', aber input funktioniert da auch
          element.addEventListener('change', e => {
            settings[key] = e.target.value;
            updateURLQueryFromSettings();
          });
        }
      }
    }

    // Initialisierung: Settings
    applyQueryParamsToSettings();
    updateUI();
    setupEventListeners();

</script>

</body>
</html>

